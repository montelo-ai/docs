---
title: Inference
description: Serverless inference for your fine-tuned models on Montelo.
icon: bolt
---

### Fine-tuned models

Our SDK exposes an OpenAI-like interface to chat with your fine-tuned models.

<CodeGroup>

  ```typescript Node
const completion = await montelo.client.chat({
    model: "model-id",
    messages: [],
});
  ```

  ```python Python
  completion = montelo.client.chat(
    model="model-id",
    messages=[],
  )
  ```

</CodeGroup>

### OpenAI & Cohere

For OpenAI and Cohere models, simply follow their documentation to learn how to do inference on your fine-tuned models.

- [OpenAI docs](https://platform.openai.com/docs/guides/fine-tuning/use-a-fine-tuned-model)

- [Cohere docs](https://docs.cohere.com/docs/troubleshooting-a-fine-tuned-model#using-your-trained-model)
