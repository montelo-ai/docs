---
title: Experiments
description: Run an experiment to evaluate your LLMs.
icon: flask
---

<video
  muted
  autoPlay
  loop
  playsInline
  className="w-full aspect-video"
  src="/assets/experiment.mp4"
></video>

## Explanation

Experiments allow you to evaluate the performance of your LLMs on specific [datasets](/datasets/datasets).
By running experiments, you can gain insights into how well your models perform on various tasks and identify areas for improvement.

### Why do I need this?

AI is non-deterministic and therefore unpredictable. The inputs and outputs you get when developing your application
will be unlike the inputs and outputs you get when you ship your application to production. In order to deploy safe AI, you need
 *careful* evaluation and monitoring during the development phase. This is where our experiments can help.

## Pre-requisites

You'll need a [datasets](/datasets/datasets) uploaded to Montelo to get started. Remember, an experiment is always
associated to a dataset.

You'll also need a `runner` and an `evaluator` function. We'll look at a classification example where the following
datapoints have been uploaded:

```typescript
await montelo.datapoints.createMany({
  dataset: dataset.slug,
  datapoints: [
    {
      input: { sentence: "That was an awesome event." },
      expectedOutput: { sentiment: "positive" }
    },
    {
      input: { sentence: "That was a bad event." },
      expectedOutput: { sentiment: "negative" }
    }
  ],
});
```

### Runner

The `runner` function runs the datapoint's input and returns the output. In the example above, the `runner` would take
the sentence as input, and has to return a sentiment. Here's the most basic example of this:

```typescript
const runner = async (sentence: string): string => {
  // call an llm to perform the classification
  const sentiment = await llmCall(sentence);
  return sentiment;
};
```

The `runner` can be *any* function. You could call an external service, an internal API, or even a really simple function
like above.

### Evaluator

The `evaluator` function takes the datapoint's expected output, and the output that the `runner` returned, and runs an evaluation on it. Again, you could run any
evaluation you want! Evaluations are just functions at the end of the day.

Note that you must return an object, not a primitive, for evaluators. Here's an example

```typescript
const evaluator = (goldSentiment: string, predictedSentiment) => {
  return {
    isCorrect: goldSentiment === predictedSentiment,
  };
};
```

Your evaluator can be any function!

- You could have an LLM act as your evaluator
- You could use string comparisons (as above)
- Levenshtein distance
- Cosine similarity
- Anything else!

We'll take the object that you return, and run statistics and create charts for you.

## Running an Experiment

Now that you have a `runner` and an `evaluator`, let's run our experiment!

```typescript
type Input = { sentence: string };
type Output = { sentiment: string };
type Eval = { isCorrect: boolean };

await montelo.experiments.createAndRun<Input, Output, Eval>({
  name: "Experiment",
  dataset: "dataset-slug",
  runner: async ({ input, metadata }) => {
    const sentiment = await runner(input.sentence);
    return { sentiment };
  },
  evaluator: async ({ input, expectedOutput, actualOutput, metadata }) => {
    return evaluator(expectedOutput.sentiment, actualOutput.sentiment);
  },
});
```

And that's all you need! Behind the scenes, Montelo will pull all the datapoints of the dataset and run the
`runner` and `evaluator` on you each datapoint.

We'll inject each datapoint's input, metadata, and expected output into your `runner` and `evaluator`, and track the
data that you return.

<video
  muted
  autoPlay
  loop
  playsInline
  className="w-full aspect-video"
  src="/assets/experiment.mp4"
></video>

### Options

You can pass in `options` to the `createAndRun` function:

<ParamField path="parallelism" type="number">
  How many datapoints to run concurrently, to speed up the time it takes to finish the experiment.
</ParamField>

<ParamField path="onlyTestSplit" type="boolean">
  Only run the experiment on the test datapoints, skipping the train datapoints.
</ParamField>

## Analyzing Experiment Results

You can analyze the results of your evaluations on our UI.

<video
  muted
  autoPlay
  loop
  playsInline
  className="w-full aspect-video"
  src="/assets/experimentanalysis.mp4"
></video>

## Debugging Runners

In your `runner`, we recommend you start a [trace](/docs/traces/traces) at the start of your runner. Montelo will pickup
that trace, and display it for you alongside the run in the UI, so you can debug what went wrong in your experiment!

<video
  muted
  autoPlay
  loop
  playsInline
  className="w-full aspect-video"
  src="/assets/debuggingexperiment.mp4"
></video>

## FAQ

### Where do traces inside the runner show up on the UI?

Traces inside a runner will only show up on the run that it's associated to, and not on the dashboard or the `Traces`
page, which are dedicated for production traces and analytics.
