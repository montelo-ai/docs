---
title: Experiments & Evals
description: Run an experiment to evaluate your fine-tuned models.
icon: flask
---

## Explanation

Experiments allow you to evaluate the performance of your LLMs on specific [datasets](/datasets/datasets).
By running experiments, you can gain insights into how well your models perform on various tasks and identify areas for improvement.

## Pre-requisites

You'll need a [datasets](/datasets/datasets) uploaded to Montelo to get started. Remember, an experiment is always
associated to a dataset.

You'll also need a `runner` and an `evaluator` function. We'll look at a classification example where the following
datapoints have been uploaded:

<CodeGroup>

```typescript Node
await montelo.datapoints.createMany({
  dataset: dataset.slug,
  datapoints: [
    {
      input: { sentence: "That was an awesome event." },
      expectedOutput: { sentiment: "positive" }
    },
    {
      input: { sentence: "That was a bad event." },
      expectedOutput: { sentiment: "negative" }
    }
  ],
});
```

```python Python
montelo.datapoints.create_many(
  dataset=dataset.slug,
  datapoints=[
    {
      "input": { "sentence": "That was an awesome event." },
      "expectedOutput": { "sentiment": "positive" }
    },
    {
      "input": { "sentence": "That was a bad event." },
      "expectedOutput": { "sentiment": "negative" }
    }
  ],
)
```
</CodeGroup>


### Runner

The `runner` function runs the datapoint's input and returns the output. In the example above, the `runner` would take
the sentence as input, and has to return a sentiment. Here's the most basic example of this:

<CodeGroup>

```typescript Node
const runner = async (sentence: string): string => {
  // call an llm to perform the classification
  const sentiment = await llmCall(sentence);
  return sentiment;
};
```

```python Python
def runner(sentence: str):
    # call an llm to perform the classification
    sentiment = llm_call(sentence)
    return dict(sentiment=sentiment)
```

</CodeGroup>


The `runner` can be *any* function. You could call an external service, an internal API, or even a really simple function
like above.

### Evaluator

The `evaluator` function takes the datapoint's expected output, and the output that the `runner` returned, and runs an evaluation on it. Again, you could run any
evaluation you want! Evaluations are just functions at the end of the day.

Note that you must return an object, not a primitive, for evaluators. Here's an example

<CodeGroup>
```typescript Node
const evaluator = (goldSentiment: string, predictedSentiment) => {
  return {
    isCorrect: goldSentiment === predictedSentiment,
  };
};
```

```python Python
def evaluator(gold_sentiment: str, predicted_sentiment: str):
    return dict(is_correct=gold_sentiment == predicted_sentiment)
```
</CodeGroup>


Your evaluator can be any function!

- You could have an LLM act as your evaluator
- You could use string comparisons (as above)
- Levenshtein distance
- Cosine similarity
- Anything else!

We'll take the object that you return, and run statistics and create charts for you.

## Running an Experiment

Now that you have a `runner` and an `evaluator`, let's run our experiment!

<CodeGroup>

```typescript Node
type Input = { sentence: string };
type Output = { sentiment: string };
type Eval = { isCorrect: boolean };

await montelo.experiments.createAndRun<Input, Output, Eval>({
  name: "Experiment",
  dataset: "dataset-slug",
  runner: async ({ input, metadata }) => {
    const sentiment = await runner(input.sentence);
    return { sentiment };
  },
  evaluator: async ({ input, expectedOutput, actualOutput, metadata }) => {
    return evaluator(expectedOutput.sentiment, actualOutput.sentiment);
  },
});
```

```python Python
montelo.experiments.create_and_run(
  name="Experiment",
  dataset="dataset-slug",
  runner=lambda input: {"sentiment": runner(input["sentence"])},
  evaluator=lambda expected_output, actual_output: evaluator(expected_output["sentiment"], actual_output["sentiment"]),
)
```

</CodeGroup>


And that's all you need! Behind the scenes, Montelo will pull all the datapoints of the dataset and run the
`runner` and `evaluator` on you each datapoint.

We'll inject each datapoint's input, metadata, and expected output into your `runner` and `evaluator`, and track the
data that you return.

<video
  muted
  autoPlay
  loop
  playsInline
  className="w-full aspect-video"
  src="/assets/experiment.mp4"
></video>

### Options

You can pass in `options` to the `createAndRun` function:

<ParamField path="parallelism" type="number">
  How many datapoints to run concurrently, to speed up the time it takes to finish the experiment.
</ParamField>

<ParamField path="onlyTestSplit" type="boolean">
  Only run the experiment on the test datapoints, skipping the train datapoints.
</ParamField>

